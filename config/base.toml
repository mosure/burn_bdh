[dataset]
cache_dir = "data"
train_split_ratio = 0.9

[dataset.tokenizer]
type = "char"
include_unknown = true
vocab_path = "vocab.json"

[training]
block_size = 512
batch_size = 32
logical_batch_size = 64
max_iters = 3000
log_frequency = 100
context_strategy = { type = "infinite" }

[optimizer]
learning_rate = 0.001
weight_decay = 0.1

[optimizer.lr_schedule]
type = "cosine"
min_lr = 0.00005
num_iters = 3000

[generation]
prompt = "To be or "
max_tokens = 2048
temperature = 1.0
top_k = 3

[model]
n_layer = 6
n_embd = 256
n_head = 4
mlp_internal_dim_multiplier = 64
dropout = 0.1
fused_kernels = true
use_alibi = true
