[dataset]
cache_dir = "data"
train_split_ratio = 0.9

[dataset.tokenizer]
type = "byte"
# include_unknown = true
# vocab_path = "vocab.json"

[training]
block_size = 512
batch_size = 16
logical_batch_size = 16
epochs = 32
log_frequency = 100
context_strategy = { type = "infinite" }

[optimizer]
learning_rate = 0.001
weight_decay = 0.05

[optimizer.lr_schedule]
type = "cosine"
min_lr = 0.00005

[generation]
prompt = "To be or "
max_tokens = 2048
temperature = 1.0
top_k = 3

[model]
n_layer = 6
n_embd = 256
n_head = 4
mlp_internal_dim_multiplier = 128
dropout = 0.1
fused_kernels = true
use_alibi = true
